{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this Question, I have implemented Multinomial Naive Bayes from Scratch \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used to preprocess strings in Abstract for the arxiv dataset\n",
    "def preprocess_string(str_arg):\n",
    "    str_arg=re.sub('[^a-z\\s]+',' ',str_arg,flags=re.IGNORECASE)\n",
    "    str_arg=re.sub('(\\s+)',' ',str_arg)\n",
    "    str_arg=str_arg.lower()\n",
    "    str_arg = unidecode(str_arg)\n",
    "    str_arg = remove_html(str_arg)\n",
    "    return str_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self):\n",
    "        #Just initializing the object here, nothing to do\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def addToBow(self,example,dict_index):\n",
    "        #Algorithm - I am storing counts of all words as per Category Index in BoW \n",
    "        example=example[0] #This is because the Example will have the shape - (1,)\n",
    "        for token_word in example.split(): #for every word in preprocessed example\n",
    "            considered_dict = self.bow_dicts[dict_index]\n",
    "            intVal = 0\n",
    "            if token_word in considered_dict:\n",
    "                intVal = considered_dict[token_word]\n",
    "                self.bow_dicts[dict_index][token_word] = intVal + 1\n",
    "            else:\n",
    "                self.bow_dicts[dict_index][token_word] = 1\n",
    "            \n",
    "    def train(self,dataset,labels):\n",
    "        self.examples=dataset\n",
    "        self.labels=labels\n",
    "        \n",
    "        self.classes = np.unique(labels)\n",
    "        \n",
    "        bag_of_word_list = []\n",
    "        for index in range(self.classes.shape[0]):\n",
    "            bag_of_word_list.append(dict())\n",
    "        self.bow_dicts = np.array(bag_of_word_list) #We want to return 0 if value does not exist\n",
    "        #For Bag of Words, we have one Dictionary for Every Category\n",
    "        \n",
    "        self.examples=np.array(self.examples)\n",
    "        self.labels=np.array(self.labels)\n",
    "            \n",
    "        #constructing BoW for each category\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "            np.apply_along_axis(self.addToBow, 1, pd.DataFrame(self.examples[self.labels==cat]),cat_index) #I am creating the BoW for each class\n",
    "\n",
    "        prob_classes=np.empty(self.classes.shape[0])\n",
    "        all_words=[]\n",
    "        cat_word_counts=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "           \n",
    "            #Calculating prior probability p(c) for each class\n",
    "            #For our particular dataset, we will observe that the prior values are the same after computation\n",
    "            number_of_label_rows = self.labels.shape[0]\n",
    "            number_of_rows_each_class = np.sum(self.labels==cat)\n",
    "            prob_classes[cat_index]=float(number_of_rows_each_class/number_of_label_rows)\n",
    "            \n",
    "            #Calculating total counts of all the words of each class \n",
    "            count=np.asarray(list(self.bow_dicts[cat_index].values()))\n",
    "            cat_word_counts[cat_index]=np.sum(count)+1\n",
    "            \n",
    "            #get all words of this category                                \n",
    "            all_words+=self.bow_dicts[cat_index].keys()\n",
    "                                                     \n",
    "        #combine all words of every category & make them unique to get vocabulary -V- of entire training set\n",
    "        self.vocab=np.unique(np.array(all_words))\n",
    "        self.vocab_length=self.vocab.shape[0]\n",
    "                                  \n",
    "        #computing denominator array (which we use later) \n",
    "        denoms_array = []\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "            denoms_array.append(cat_word_counts[cat_index]+self.vocab_length+1)\n",
    "        denoms = np.asarray(denoms_array) #We typecast\n",
    "        \n",
    "        #Storing BoW, Probability and Denom for each category (in this order):\n",
    "        arr_new = []\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "            arr_new.append([self.bow_dicts[cat_index],prob_classes[cat_index],denoms[cat_index]])\n",
    "        self.cats_info = np.array(arr_new)\n",
    "                                              \n",
    "                                              \n",
    "    def getExampleProb(self,test_example):                                                                \n",
    "        likelihood_prob=np.zeros(self.classes.shape[0]) #to store probability w.r.t each class\n",
    "        post_prob=np.zeros(self.classes.shape[0])\n",
    "        #finding probability w.r.t each class of the given test example\n",
    "        for cat_index,cat in enumerate(self.classes): \n",
    "            for test_token in test_example.split():\n",
    "                word_map = self.cats_info[cat_index][0]\n",
    "                if test_token in word_map: \n",
    "                    c_val = self.cats_info[cat_index][0].get(test_token)\n",
    "                else:\n",
    "                    c_val = 0 #Take 0 if value is absent \n",
    "                test_token_counts=c_val+1 #If we don't add +1, we are going to run into Log0 or undefined statements\n",
    "                denominator_values_arr = self.cats_info[cat_index][2]\n",
    "                test_token_prob=float(test_token_counts/denominator_values_arr)                       \n",
    "                likelihood_prob[cat_index]=likelihood_prob[cat_index]+np.log10(test_token_prob) #log stops underflow\n",
    "                                              \n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "            post_prob[cat_index]=likelihood_prob[cat_index]+np.log10(self.cats_info[cat_index][1])                                  \n",
    "      \n",
    "        return post_prob\n",
    "    \n",
    "   \n",
    "    def test(self,test_set):\n",
    "        predictions=[]\n",
    "        for example in test_set:                                \n",
    "            post_prob=self.getExampleProb(example)\n",
    "            predictions.append(self.classes[np.argmax(post_prob)])\n",
    "                \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('E:/kaggle1/train.csv') #Please Replace here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I am dropping duplicates if there are any\n",
    "train_df=train_df.drop_duplicates()\n",
    "\n",
    "#Preprocess the string\n",
    "train_df['Abstract'] =  train_df['Abstract'].apply(preprocess_string)\n",
    "\n",
    "#Preprocess some more!\n",
    "train_df['Abstract'] = train_df['Abstract'].apply(lambda x: x.replace('&gt;', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forming the DataSets\n",
    "#80-20 Split\n",
    "total_rows = train_df.shape[0]\n",
    "eighty_p = 80/100 * total_rows\n",
    "remaining = total_rows - eighty_p\n",
    "\n",
    "X_training = train_df['Abstract'][0:int(eighty_p)]\n",
    "X_testing = train_df['Abstract'][int(eighty_p):]\n",
    "Y_Training = train_df['Category'][0:int(eighty_p)]\n",
    "Y_Testing = train_df['Category'][int(eighty_p):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=NaiveBayes() #instantiate a NB class object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.train(X_training,Y_Training) #start tarining by calling the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclasses=nb.test(X_testing) #get predcitions for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how many predcitions actually match original test labels\n",
    "test_acc=np.sum(pclasses==Y_Testing)/float(Y_Testing.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Examples:  1500\n",
      "Test Set Accuracy:  78.93333333333334 %\n"
     ]
    }
   ],
   "source": [
    "print (\"Test Set Examples: \",Y_Testing.shape[0])\n",
    "print (\"Test Set Accuracy: \",test_acc*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference I used code and guidance from while coding the class: \n",
    "\n",
    "#1. https://towardsdatascience.com/unfolding-na%C3%AFve-bayes-from-scratch-2e86dcae4b01\n",
    "#2. https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/    \n",
    "#3. https://github.com/aishajv/Unfolding-Naive-Bayes-from-Scratch/blob/master/%23%20Unfolding%20Na%C3%AFve%20Bayes%20from%20Scratch!%20Take-2%20%F0%9F%8E%AC.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let us run this against a CSV that can work in Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('E:/kaggle1/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WE MUST apply the same preprocessing to the test dataframe. Otherwise we are making a grave mistake and might get worse predictions. \n",
    "\n",
    "#I am dropping duplicates if there are any\n",
    "test_df=test_df.drop_duplicates()\n",
    "\n",
    "#Preprocess the string\n",
    "test_df['Abstract'] =  test_df['Abstract'].apply(preprocess_string)\n",
    "\n",
    "#Preprocess some more!\n",
    "test_df['Abstract'] = test_df['Abstract'].apply(lambda x: x.replace('&gt;', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFrame = test_df['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclasses2=nb.test(testFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['stat.ML', 'astro-ph.SR', 'astro-ph.SR', ..., 'astro-ph.GA',\n",
       "       'gr-qc', 'cond-mat.mes-hall'], dtype='<U17')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pclasses2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_preds = pd.DataFrame(pclasses2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_preds = df_final_preds.rename(columns={0: \"Category\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_preds[\"Id\"] = df_final_preds.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_titles = [\"Id\",\"Category\"]\n",
    "df_final_preds=df_final_preds.reindex(columns=columns_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_preds.to_csv(\"C:/kaggleMultiNB\", index = False) #Generates File. It can be used to submit on Kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: To improve performance, we can train on the whole dataset instead of 80%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
